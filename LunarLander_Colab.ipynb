{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a408afa3",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02262e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install gymnasium[box2d] imageio matplotlib torch numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e25669",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e85e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import collections\n",
    "from datetime import datetime\n",
    "\n",
    "import gymnasium as gym\n",
    "import imageio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc65a26c",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd55de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_name: str, seed: int = 42, render_mode=None):\n",
    "    \"\"\"Create and seed environment\"\"\"\n",
    "    env = gym.make(env_name, render_mode=render_mode)\n",
    "    env.reset(seed=seed)\n",
    "    env.action_space.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccf19ad",
   "metadata": {},
   "source": [
    "## Q-Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b554c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"2-layer MLP for Q(s,a) approximation\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45994cdb",
   "metadata": {},
   "source": [
    "## Replay Buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d8841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Standard replay buffer\"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int = 100_000):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            np.array(states, dtype=np.float32),\n",
    "            np.array(actions, dtype=np.int64),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(next_states, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.float32),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    \"\"\"Proportional priority replay buffer\"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int = 100_000, alpha: float = 0.6):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.pos = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        max_prio = self.priorities.max() if self.buffer else 1.0\n",
    "\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((state, action, reward, next_state, done))\n",
    "        else:\n",
    "            self.buffer[self.pos] = (state, action, reward, next_state, done)\n",
    "\n",
    "        self.priorities[self.pos] = max_prio\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size: int, beta: float = 0.4):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[: self.pos]\n",
    "\n",
    "        probs = prios ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        states, actions, rewards, next_states, dones = zip(*samples)\n",
    "\n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "\n",
    "        return (\n",
    "            np.array(states, dtype=np.float32),\n",
    "            np.array(actions, dtype=np.int64),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(next_states, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.float32),\n",
    "            indices,\n",
    "            np.array(weights, dtype=np.float32),\n",
    "        )\n",
    "\n",
    "    def update_priorities(self, indices, new_priorities):\n",
    "        for idx, prio in zip(indices, new_priorities):\n",
    "            self.priorities[idx] = float(prio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfece0d",
   "metadata": {},
   "source": [
    "## DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d01c66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"Agent supporting DQN, DDQN, and PER\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        algo_type: str = \"dqn\",\n",
    "        gamma: float = 0.99,\n",
    "        lr: float = 1e-3,\n",
    "        batch_size: int = 64,\n",
    "        buffer_capacity: int = 100_000,\n",
    "        min_buffer_size: int = 10_000,\n",
    "        target_update_freq: int = 1_000,\n",
    "        device: str = None,\n",
    "        eps_decay_frames: int = 250_000,\n",
    "    ):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.min_buffer_size = min_buffer_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.algo_type = algo_type.lower()\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.q_net = QNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.target_net = QNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=50000, gamma=0.9)\n",
    "\n",
    "        self.loss_fn = nn.SmoothL1Loss(\n",
    "            reduction=\"none\" if self.algo_type == \"per\" else \"mean\"\n",
    "        )\n",
    "\n",
    "        if self.algo_type == \"per\":\n",
    "            self.buffer = PrioritizedReplayBuffer(capacity=buffer_capacity)\n",
    "            self.beta_start = 0.4\n",
    "            self.beta_frames = 200_000\n",
    "        else:\n",
    "            self.buffer = ReplayBuffer(capacity=buffer_capacity)\n",
    "\n",
    "        self.eps_start = 1.0\n",
    "        self.eps_end = 0.05\n",
    "        self.eps_decay = eps_decay_frames\n",
    "        self.frame_idx = 0\n",
    "        self.training_steps = 0\n",
    "\n",
    "    def epsilon(self) -> float:\n",
    "        return self.eps_end + (self.eps_start - self.eps_end) * math.exp(\n",
    "            -1.0 * self.frame_idx / self.eps_decay\n",
    "        )\n",
    "\n",
    "    def select_action(self, state: np.ndarray) -> int:\n",
    "        \"\"\"Epsilon-greedy action selection\"\"\"\n",
    "        self.frame_idx += 1\n",
    "        if random.random() < self.epsilon():\n",
    "            return random.randrange(self.action_dim)\n",
    "\n",
    "        state_t = torch.tensor(\n",
    "            state, dtype=torch.float32, device=self.device\n",
    "        ).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_net(state_t)\n",
    "        return int(q_values.argmax(dim=1).item())\n",
    "\n",
    "    def greedy_action(self, state: np.ndarray) -> int:\n",
    "        \"\"\"Greedy action selection\"\"\"\n",
    "        state_t = torch.tensor(\n",
    "            state, dtype=torch.float32, device=self.device\n",
    "        ).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_net(state_t)\n",
    "        return int(q_values.argmax(dim=1).item())\n",
    "\n",
    "    def push(self, *transition):\n",
    "        self.buffer.push(*transition)\n",
    "\n",
    "    def can_update(self) -> bool:\n",
    "        return len(self.buffer) >= self.min_buffer_size\n",
    "\n",
    "    def compute_td_target(\n",
    "        self, rewards: torch.Tensor, next_states: torch.Tensor, dones: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Compute TD targets for DQN/DDQN/PER\"\"\"\n",
    "        with torch.no_grad():\n",
    "            next_q_target = self.target_net(next_states)\n",
    "\n",
    "            if self.algo_type in (\"ddqn\", \"per\"):\n",
    "                next_q_online = self.q_net(next_states)\n",
    "                next_actions = next_q_online.argmax(dim=1, keepdim=True)\n",
    "                next_q = next_q_target.gather(1, next_actions).squeeze(1)\n",
    "            else:\n",
    "                next_q, _ = next_q_target.max(dim=1)\n",
    "\n",
    "            td_target = rewards + self.gamma * (1.0 - dones) * next_q\n",
    "        return td_target\n",
    "\n",
    "    def update(self):\n",
    "        if not self.can_update():\n",
    "            return None\n",
    "\n",
    "        self.training_steps += 1\n",
    "\n",
    "        if self.algo_type == \"per\":\n",
    "            beta = min(\n",
    "                1.0,\n",
    "                self.beta_start\n",
    "                + self.training_steps * (1.0 - self.beta_start) / self.beta_frames,\n",
    "            )\n",
    "            (\n",
    "                states,\n",
    "                actions,\n",
    "                rewards,\n",
    "                next_states,\n",
    "                dones,\n",
    "                indices,\n",
    "                weights,\n",
    "            ) = self.buffer.sample(self.batch_size, beta)\n",
    "            weights = torch.tensor(\n",
    "                weights, dtype=torch.float32, device=self.device\n",
    "            )\n",
    "        else:\n",
    "            states, actions, rewards, next_states, dones = self.buffer.sample(\n",
    "                self.batch_size\n",
    "            )\n",
    "            weights = torch.ones(self.batch_size, device=self.device)\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32, device=self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64, device=self.device).unsqueeze(\n",
    "            1\n",
    "        )\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        next_states = torch.tensor(\n",
    "            next_states, dtype=torch.float32, device=self.device\n",
    "        )\n",
    "        dones = torch.tensor(dones, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        q_values = self.q_net(states).gather(1, actions).squeeze(1)\n",
    "\n",
    "        td_target = self.compute_td_target(rewards, next_states, dones)\n",
    "\n",
    "        loss_tensor = self.loss_fn(q_values, td_target)\n",
    "        loss = (loss_tensor * weights).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.q_net.parameters(), max_norm=10.0)\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "\n",
    "        if self.algo_type == \"per\":\n",
    "            new_priorities = loss_tensor.detach().cpu().numpy() + 1e-6\n",
    "            self.buffer.update_priorities(indices, new_priorities)\n",
    "\n",
    "        if self.training_steps % self.target_update_freq == 0:\n",
    "            self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "        return float(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c226e00",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d419f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(\n",
    "    env_name: str,\n",
    "    algo_type: str,\n",
    "    num_episodes: int = 600,\n",
    "    seed: int = 42,\n",
    ") -> tuple:\n",
    "    \"\"\"Train agent and return rewards\"\"\"\n",
    "\n",
    "    env = make_env(env_name, seed=seed, render_mode=None)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    eps_decay_frames = max(250_000, num_episodes * 400)\n",
    "\n",
    "    agent = DQNAgent(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        algo_type=algo_type,\n",
    "        gamma=0.99,\n",
    "        lr=1e-3,\n",
    "        batch_size=64,\n",
    "        buffer_capacity=100_000,\n",
    "        min_buffer_size=10_000,\n",
    "        target_update_freq=1_000,\n",
    "        eps_decay_frames=eps_decay_frames,\n",
    "    )\n",
    "\n",
    "    episode_rewards = []\n",
    "\n",
    "    print(f\"\\n=== Training {algo_type.upper()} on {env_name} for {num_episodes} episodes ===\")\n",
    "    print(f\"Epsilon decay frames: {eps_decay_frames:,}\")\n",
    "\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            agent.push(state, action, reward, next_state, float(done))\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if agent.can_update():\n",
    "                agent.update()\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "        if episode % 20 == 0:\n",
    "            last_mean = np.mean(episode_rewards[-20:])\n",
    "            current_lr = agent.optimizer.param_groups[0]['lr']\n",
    "            print(\n",
    "                f\"Episode {episode:4d} | \"\n",
    "                f\"avg reward (last 20): {last_mean:7.2f} | \"\n",
    "                f\"epsilon: {agent.epsilon():.3f} | \"\n",
    "                f\"lr: {current_lr:.6f}\"\n",
    "            )\n",
    "\n",
    "    env.close()\n",
    "    return agent, episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96918a70",
   "metadata": {},
   "source": [
    "## Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bd0650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(results: dict, save_path: str = \"learning_curves.png\"):\n",
    "    \"\"\"Plot reward curves\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for label, rewards in results.items():\n",
    "        rewards = np.array(rewards)\n",
    "        window = 20\n",
    "        if len(rewards) >= window:\n",
    "            smooth = np.convolve(rewards, np.ones(window) / window, mode=\"valid\")\n",
    "            plt.plot(\n",
    "                range(window, len(rewards) + 1),\n",
    "                smooth,\n",
    "                label=f\"{label} (moving avg)\",\n",
    "            )\n",
    "        plt.plot(np.arange(1, len(rewards) + 1), rewards, alpha=0.25, linestyle=\"--\")\n",
    "\n",
    "    plt.axhline(200, color=\"grey\", linestyle=\":\", label=\"Solved threshold (â‰ˆ200)\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episodic Return\")\n",
    "    plt.title(\"LunarLander-v3: DQN vs DDQN vs PER\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved learning curves to {save_path}\")\n",
    "\n",
    "\n",
    "def generate_gif(\n",
    "    agent,\n",
    "    env_name: str,\n",
    "    filename: str,\n",
    "    seed: int = 0,\n",
    "    episodes: int = 3,\n",
    "):\n",
    "    \"\"\"Generate GIF of trained agent\"\"\"\n",
    "    env = make_env(env_name, seed=seed, render_mode=\"rgb_array\")\n",
    "    frames = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done:\n",
    "            frame = env.render()\n",
    "            frames.append(frame)\n",
    "\n",
    "            action = agent.greedy_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "            step += 1\n",
    "\n",
    "    env.close()\n",
    "    imageio.mimsave(filename, frames, fps=30)\n",
    "    print(f\"Saved GIF for policy to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d98e016",
   "metadata": {},
   "source": [
    "## Configuration and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a4b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "ENV_NAME = \"LunarLander-v3\"\n",
    "NUM_EPISODES = 1000  # Edit the num of episodes you want to run here\n",
    "TEST_NAME = \"Test_Colab_1000eps\"\n",
    "\n",
    "# Create output directory\n",
    "output_dir = f\"/content/{TEST_NAME}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Starting Training Run: {TEST_NAME}\")\n",
    "print(f\"Episodes per algorithm: {NUM_EPISODES}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8f9098",
   "metadata": {},
   "source": [
    "## Train All Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f038945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "trained_agents = {}\n",
    "\n",
    "for algo in [\"dqn\", \"ddqn\", \"per\"]:\n",
    "    agent, rewards = train_agent(ENV_NAME, algo_type=algo, num_episodes=NUM_EPISODES)\n",
    "    results[algo.upper()] = rewards\n",
    "    trained_agents[algo.upper()] = agent\n",
    "\n",
    "    print(\n",
    "        f\"{algo.upper()} final mean reward over last 50 episodes: \"\n",
    "        f\"{np.mean(rewards[-50:]):.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e52440",
   "metadata": {},
   "source": [
    "## Generate Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0df0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(results, save_path=f\"{output_dir}/learning_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef50013",
   "metadata": {},
   "source": [
    "## Generate GIFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061b22e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_gif(trained_agents[\"DQN\"], ENV_NAME, f\"{output_dir}/dqn_agent.gif\")\n",
    "generate_gif(trained_agents[\"DDQN\"], ENV_NAME, f\"{output_dir}/ddqn_agent.gif\")\n",
    "generate_gif(trained_agents[\"PER\"], ENV_NAME, f\"{output_dir}/per_agent.gif\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training Complete: {TEST_NAME}\")\n",
    "print(f\"All results saved to: {output_dir}/\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b20fd3f",
   "metadata": {},
   "source": [
    "## Display Results\n",
    "\n",
    "You can download the generated files from the Files panel on the left, or use the code below to display the GIFs inline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebbeda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Display GIFs\n",
    "print(\"DQN Agent:\")\n",
    "display(Image(filename=f\"{output_dir}/dqn_agent.gif\"))\n",
    "\n",
    "print(\"\\nDDQN Agent:\")\n",
    "display(Image(filename=f\"{output_dir}/ddqn_agent.gif\"))\n",
    "\n",
    "print(\"\\nPER Agent:\")\n",
    "display(Image(filename=f\"{output_dir}/per_agent.gif\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef260d2",
   "metadata": {},
   "source": [
    "## Download Results\n",
    "\n",
    "Run this cell to download all results as a zip file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d878b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the results\n",
    "!zip -r {output_dir}.zip {output_dir}\n",
    "\n",
    "# Download the zip file\n",
    "from google.colab import files\n",
    "files.download(f\"{output_dir}.zip\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
