{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45c220b9",
   "metadata": {},
   "source": [
    "# LunarLander-v3 – DQN / DDQN / PER\n",
    "**Author: Filipe Ramos (CIS2719 Coursework 2)**\n",
    "\n",
    "This notebook trains three deep RL agents on the Gymnasium LunarLander-v3 task using:\n",
    "- **DQN** (Deep Q-Network)\n",
    "- **DDQN** (Double DQN)\n",
    "- **PER** (Prioritised Experience Replay + Double DQN targets)\n",
    "\n",
    "Each agent undergoes the same number of training episodes for fair comparison:\n",
    "- 500, 1000, 2500, 5000, 10000\n",
    "\n",
    "**Output files:**\n",
    "- `learning_curves.png` - shows episodic return against number of episodes\n",
    "- `dqn_agent.gif` - Displays DQN post training\n",
    "- `ddqn_agent.gif` - Displays DDQN post training\n",
    "- `per_agent.gif` - Displays PER post training\n",
    "- `results_table.txt` - Displays training time, inference time, final average reward\n",
    "\n",
    "For more specific information, refer to the README.md file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a408afa3",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02262e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install system dependencies for Box2D\n",
    "!apt-get update -qq\n",
    "!apt-get install -y swig build-essential python3-dev\n",
    "\n",
    "# Install required packages\n",
    "!pip install gymnasium imageio matplotlib torch numpy\n",
    "!pip install box2d-py==2.3.8\n",
    "!pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e25669",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e85e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import collections\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import gymnasium as gym\n",
    "import imageio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc65a26c",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd55de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_name: str, seed: int = 42, render_mode=None):\n",
    "    \"\"\"Create and seed environment\"\"\"\n",
    "    env = gym.make(env_name, render_mode=render_mode)\n",
    "    env.reset(seed=seed)\n",
    "    env.action_space.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccf19ad",
   "metadata": {},
   "source": [
    "## Q-Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b554c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"2-layer MLP for Q(s,a) approximation\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45994cdb",
   "metadata": {},
   "source": [
    "## Replay Buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d8841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Standard replay buffer\"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int = 100_000):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            np.array(states, dtype=np.float32),\n",
    "            np.array(actions, dtype=np.int64),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(next_states, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.float32),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    \"\"\"Proportional priority replay buffer\"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int = 100_000, alpha: float = 0.6):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.pos = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        max_prio = self.priorities.max() if self.buffer else 1.0\n",
    "\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((state, action, reward, next_state, done))\n",
    "        else:\n",
    "            self.buffer[self.pos] = (state, action, reward, next_state, done)\n",
    "\n",
    "        self.priorities[self.pos] = max_prio\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size: int, beta: float = 0.4):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[: self.pos]\n",
    "\n",
    "        probs = prios ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        states, actions, rewards, next_states, dones = zip(*samples)\n",
    "\n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "\n",
    "        return (\n",
    "            np.array(states, dtype=np.float32),\n",
    "            np.array(actions, dtype=np.int64),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(next_states, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.float32),\n",
    "            indices,\n",
    "            np.array(weights, dtype=np.float32),\n",
    "        )\n",
    "\n",
    "    def update_priorities(self, indices, new_priorities):\n",
    "        for idx, prio in zip(indices, new_priorities):\n",
    "            self.priorities[idx] = float(prio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfece0d",
   "metadata": {},
   "source": [
    "## DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d01c66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    # Agent supporting DQN, DDQN, and PER\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        algo_type: str = \"dqn\",\n",
    "        gamma: float = 0.99,\n",
    "        lr: float = 2.5e-4,\n",
    "        batch_size: int = 32,\n",
    "        buffer_capacity: int = 500_000,\n",
    "        min_buffer_size: int = 10_000,\n",
    "        target_update_freq: int = 10_000,\n",
    "        device: str | None = None,\n",
    "        eps_decay_frames: int = 250_000,\n",
    "    ):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.min_buffer_size = min_buffer_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.algo_type = algo_type.lower()\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.q_net = QNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.target_net = QNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=50000, gamma=0.9)\n",
    "\n",
    "        self.loss_fn = nn.SmoothL1Loss(\n",
    "            reduction=\"none\" if self.algo_type == \"per\" else \"mean\"\n",
    "        )\n",
    "\n",
    "        if self.algo_type == \"per\":\n",
    "            self.buffer = PrioritizedReplayBuffer(capacity=buffer_capacity)\n",
    "            self.beta_start = 0.4\n",
    "            self.beta_frames = 200_000\n",
    "        else:\n",
    "            self.buffer = ReplayBuffer(capacity=buffer_capacity)\n",
    "\n",
    "        self.eps_start = 1.0\n",
    "        self.eps_end = 0.05\n",
    "        self.eps_decay = eps_decay_frames\n",
    "        self.frame_idx = 0\n",
    "\n",
    "        self.training_steps = 0\n",
    "\n",
    "    def epsilon(self) -> float:\n",
    "        return self.eps_end + (self.eps_start - self.eps_end) * math.exp(\n",
    "            -1.0 * self.frame_idx / self.eps_decay\n",
    "        )\n",
    "\n",
    "    def select_action(self, state: np.ndarray) -> int:\n",
    "        \"\"\"Epsilon-greedy action selection\"\"\"\n",
    "        self.frame_idx += 1\n",
    "        if random.random() < self.epsilon():\n",
    "            return random.randrange(self.action_dim)\n",
    "\n",
    "        state_t = torch.tensor(\n",
    "            state, dtype=torch.float32, device=self.device\n",
    "        ).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_net(state_t)\n",
    "        return int(q_values.argmax(dim=1).item())\n",
    "\n",
    "    def greedy_action(self, state: np.ndarray) -> int:\n",
    "        \"\"\"Greedy action selection\"\"\"\n",
    "        state_t = torch.tensor(\n",
    "            state, dtype=torch.float32, device=self.device\n",
    "        ).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_net(state_t)\n",
    "        return int(q_values.argmax(dim=1).item())\n",
    "\n",
    "    def push(self, *transition):\n",
    "        self.buffer.push(*transition)\n",
    "\n",
    "    def can_update(self) -> bool:\n",
    "        return len(self.buffer) >= self.min_buffer_size\n",
    "\n",
    "    def compute_td_target(\n",
    "        self, rewards: torch.Tensor, next_states: torch.Tensor, dones: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Compute TD targets for DQN/DDQN/PER\"\"\"\n",
    "        with torch.no_grad():\n",
    "            next_q_target = self.target_net(next_states)\n",
    "\n",
    "            if self.algo_type in (\"ddqn\", \"per\"):\n",
    "                next_q_online = self.q_net(next_states)\n",
    "                next_actions = next_q_online.argmax(dim=1, keepdim=True)\n",
    "                next_q = next_q_target.gather(1, next_actions).squeeze(1)\n",
    "            else:\n",
    "                next_q, _ = next_q_target.max(dim=1)\n",
    "\n",
    "            td_target = rewards + self.gamma * (1.0 - dones) * next_q\n",
    "        return td_target\n",
    "\n",
    "    def update(self):\n",
    "        if not self.can_update():\n",
    "            return None\n",
    "\n",
    "        self.training_steps += 1\n",
    "\n",
    "        if self.algo_type == \"per\":\n",
    "            beta = min(\n",
    "                1.0,\n",
    "                self.beta_start\n",
    "                + self.training_steps * (1.0 - self.beta_start) / self.beta_frames,\n",
    "            )\n",
    "            (\n",
    "                states,\n",
    "                actions,\n",
    "                rewards,\n",
    "                next_states,\n",
    "                dones,\n",
    "                indices,\n",
    "                weights,\n",
    "            ) = self.buffer.sample(self.batch_size, beta)\n",
    "            weights = torch.tensor(\n",
    "                weights, dtype=torch.float32, device=self.device\n",
    "            )\n",
    "        else:\n",
    "            states, actions, rewards, next_states, dones = self.buffer.sample(\n",
    "                self.batch_size\n",
    "            )\n",
    "            weights = torch.ones(self.batch_size, device=self.device)\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32, device=self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64, device=self.device).unsqueeze(\n",
    "            1\n",
    "        )\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        next_states = torch.tensor(\n",
    "            next_states, dtype=torch.float32, device=self.device\n",
    "        )\n",
    "        dones = torch.tensor(dones, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        q_values = self.q_net(states).gather(1, actions).squeeze(1)\n",
    "\n",
    "        td_target = self.compute_td_target(rewards, next_states, dones)\n",
    "\n",
    "        loss_tensor = self.loss_fn(q_values, td_target)\n",
    "        loss = (loss_tensor * weights).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.q_net.parameters(), max_norm=10.0)\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "\n",
    "        if self.algo_type == \"per\":\n",
    "            new_priorities = loss_tensor.detach().cpu().numpy() + 1e-6\n",
    "            self.buffer.update_priorities(indices, new_priorities)\n",
    "\n",
    "        if self.training_steps % self.target_update_freq == 0:\n",
    "            self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "        return float(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c226e00",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d419f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(\n",
    "    agent,\n",
    "    env_name: str,\n",
    "    num_episodes: int = 10,\n",
    "    seed: int = 999,\n",
    ") -> tuple:\n",
    "\n",
    "    env = make_env(env_name, seed=seed, render_mode=None)\n",
    "    episode_rewards = []\n",
    "    episode_times = []\n",
    "\n",
    "    print(f\"\\n--- Evaluation Phase: {num_episodes} episodes (greedy policy) ---\")\n",
    "\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "\n",
    "        episode_start = time.time()\n",
    "\n",
    "        while not done:\n",
    "            action = agent.greedy_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        episode_time = time.time() - episode_start\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_times.append(episode_time)\n",
    "\n",
    "        print(f\"Eval Episode {episode:2d}: Reward = {total_reward:7.2f}, Time = {episode_time:.3f}s\")\n",
    "\n",
    "    avg_inference_time = np.mean(episode_times)\n",
    "    env.close()\n",
    "\n",
    "    return episode_rewards, avg_inference_time\n",
    "\n",
    "\n",
    "def train_agent(\n",
    "    env_name: str,\n",
    "    algo_type: str,\n",
    "    num_episodes: int = 600,\n",
    "    seed: int = 42,\n",
    ") -> tuple:\n",
    "    #Train agent and return (agent, episode_rewards, training_time)\n",
    "\n",
    "    env = make_env(env_name, seed=seed, render_mode=None)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    total_timesteps = num_episodes * 200\n",
    "    eps_decay_frames = int(0.1 * total_timesteps)\n",
    "\n",
    "    agent = DQNAgent(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        algo_type=algo_type,\n",
    "        gamma=0.99,\n",
    "        lr=2.5e-4,\n",
    "        batch_size=32,\n",
    "        buffer_capacity=500_000,\n",
    "        min_buffer_size=10_000,\n",
    "        target_update_freq=10_000,\n",
    "        eps_decay_frames=eps_decay_frames,\n",
    "    )\n",
    "\n",
    "    episode_rewards = []\n",
    "\n",
    "    print(f\"\\nTraining {algo_type.upper()} on {env_name}\")\n",
    "    print(f\"Episodes: {num_episodes} | Epsilon decay frames: {eps_decay_frames:,} (~10% of {total_timesteps:,} timesteps)\")\n",
    "    print(f\"Hyperparameters: lr={agent.optimizer.param_groups[0]['lr']}, \"\n",
    "          f\"gamma={agent.gamma}, batch_size={agent.batch_size}, buffer={500_000:,}\")\n",
    "\n",
    "    training_start = time.time()\n",
    "\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            agent.push(state, action, reward, next_state, float(done))\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if agent.can_update():\n",
    "                agent.update()\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "        # Print progress every 20 episodes with moving average (last 20 episodes)\n",
    "        if episode % 20 == 0:\n",
    "            last_mean = np.mean(episode_rewards[-20:])\n",
    "            current_lr = agent.optimizer.param_groups[0]['lr']\n",
    "            elapsed = time.time() - training_start\n",
    "            print(\n",
    "                f\"Episode {episode:4d} | \"\n",
    "                f\"Reward (moving avg, window=20): {last_mean:7.2f} | \"\n",
    "                f\"Epsilon: {agent.epsilon():.3f} | \"\n",
    "                f\"LR: {current_lr:.6f} | \"\n",
    "                f\"Time: {elapsed:.1f}s\"\n",
    "            )\n",
    "\n",
    "    training_time = time.time() - training_start\n",
    "\n",
    "    print(f\"\\nTraining completed in {training_time:.2f}s ({training_time/60:.2f} min)\")\n",
    "    print(f\"Final 100-episode average: {np.mean(episode_rewards[-100:]):.2f}\")\n",
    "\n",
    "    env.close()\n",
    "    return agent, episode_rewards, training_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96918a70",
   "metadata": {},
   "source": [
    "## Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bd0650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(results: dict, save_path: str = \"learning_curves.png\"):\n",
    "    \"\"\"Plot reward curves\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for label, rewards in results.items():\n",
    "        rewards = np.array(rewards)\n",
    "        window = 20\n",
    "        if len(rewards) >= window:\n",
    "            smooth = np.convolve(rewards, np.ones(window) / window, mode=\"valid\")\n",
    "            plt.plot(\n",
    "                range(window, len(rewards) + 1),\n",
    "                smooth,\n",
    "                label=f\"{label} (moving avg)\",\n",
    "            )\n",
    "        plt.plot(np.arange(1, len(rewards) + 1), rewards, alpha=0.25, linestyle=\"--\")\n",
    "\n",
    "    plt.axhline(200, color=\"grey\", linestyle=\":\", label=\"Solved threshold (≈200)\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episodic Return\")\n",
    "    plt.title(\"LunarLander-v3: DQN vs DDQN vs PER\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"Saved learning curves to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbbd114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gif(\n",
    "    agent,\n",
    "    env_name: str,\n",
    "    filename: str,\n",
    "    seed: int = 0,\n",
    "    episodes: int = 3,\n",
    "):\n",
    "    \"\"\"Generate GIF of trained agent\"\"\"\n",
    "    env = make_env(env_name, seed=seed, render_mode=\"rgb_array\")\n",
    "    frames = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done:\n",
    "            frame = env.render()\n",
    "            frames.append(frame)\n",
    "\n",
    "            action = agent.greedy_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "            step += 1\n",
    "\n",
    "    env.close()\n",
    "    imageio.mimsave(filename, frames, fps=30)\n",
    "    print(f\"Saved GIF for policy to {filename}\")\n",
    "\n",
    "\n",
    "def print_results_table(results_data: dict, output_dir: str):\n",
    "    # Print and save comparison table for all algorithms\n",
    "\n",
    "    print(f\"\\nRESULTS COMPARISON TABLE\")\n",
    "    print(f\"{'Algorithm':<12} | {'Training Time':<15} | {'Inference Time':<17} | \"\n",
    "          f\"{'Final Train Avg':<17} | {'Eval Avg (10 eps)':<18}\")\n",
    "    print(f\"{'-'*12}-|-{'-'*15}-|-{'-'*17}-|-{'-'*17}-|-{'-'*18}\")\n",
    "\n",
    "    table_lines = []\n",
    "    table_lines.append(\"\\nRESULTS COMPARISON TABLE\")\n",
    "    table_lines.append(f\"{'Algorithm':<12} | {'Training Time':<15} | {'Inference Time':<17} | \"\n",
    "                      f\"{'Final Train Avg':<17} | {'Eval Avg (10 eps)':<18}\")\n",
    "    table_lines.append(\"-\" * 100)\n",
    "\n",
    "    for algo in [\"DQN\", \"DDQN\", \"PER\"]:\n",
    "        data = results_data[algo]\n",
    "        train_time_str = f\"{data['training_time']:.2f}s ({data['training_time']/60:.2f}m)\"\n",
    "        inference_time_str = f\"{data['inference_time']:.4f}s\"\n",
    "        final_train_avg = f\"{data['final_train_avg']:.2f}\"\n",
    "        eval_avg = f\"{data['eval_avg']:.2f} ± {data['eval_std']:.2f}\"\n",
    "\n",
    "        line = (f\"{algo:<12} | {train_time_str:<15} | {inference_time_str:<17} | \"\n",
    "                f\"{final_train_avg:<17} | {eval_avg:<18}\")\n",
    "        print(line)\n",
    "        table_lines.append(line)\n",
    "\n",
    "    # Save to file\n",
    "    with open(f\"{output_dir}/results_table.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(table_lines))\n",
    "\n",
    "    print(f\"\\nResults table saved to {output_dir}/results_table.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d98e016",
   "metadata": {},
   "source": [
    "## Configuration and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a4b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "ENV_NAME = \"LunarLander-v3\"\n",
    "\n",
    "# Configuration: Edit episode count and test name here\n",
    "NUM_EPISODES = 1000  # Edit the num of episodes you want to run here\n",
    "TEST_NAME = \"Test_Colab_1000eps\"\n",
    "NUM_EVAL_EPISODES = 10  # Number of evaluation episodes after training\n",
    "\n",
    "# Create output directory\n",
    "output_dir = f\"/content/{TEST_NAME}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"\\nEXPERIMENT: {TEST_NAME}\")\n",
    "print(f\"Environment: {ENV_NAME}\")\n",
    "print(f\"Training episodes per algorithm: {NUM_EPISODES}\")\n",
    "print(f\"Evaluation episodes: {NUM_EVAL_EPISODES}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Moving average window: 20 episodes\")\n",
    "print(f\"Solved threshold: ~200 average reward\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8f9098",
   "metadata": {},
   "source": [
    "## Train All Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f038945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "trained_agents = {}\n",
    "results_data = {}\n",
    "\n",
    "# Train all algorithms\n",
    "for algo in [\"dqn\", \"ddqn\", \"per\"]:\n",
    "    agent, rewards, train_time = train_agent(\n",
    "        ENV_NAME, algo_type=algo, num_episodes=NUM_EPISODES\n",
    "    )\n",
    "    results[algo.upper()] = rewards\n",
    "    trained_agents[algo.upper()] = agent\n",
    "\n",
    "    # Evaluate trained agent\n",
    "    eval_rewards, inference_time = evaluate_agent(\n",
    "        agent, ENV_NAME, num_episodes=NUM_EVAL_EPISODES\n",
    "    )\n",
    "\n",
    "    # Store results for table\n",
    "    results_data[algo.upper()] = {\n",
    "        'training_time': train_time,\n",
    "        'inference_time': inference_time,\n",
    "        'final_train_avg': np.mean(rewards[-100:]),\n",
    "        'eval_avg': np.mean(eval_rewards),\n",
    "        'eval_std': np.std(eval_rewards),\n",
    "        'eval_rewards': eval_rewards,\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{algo.upper()} Summary:\")\n",
    "    print(f\"  Training time: {train_time:.2f}s ({train_time/60:.2f} min)\")\n",
    "    print(f\"  Final 100-episode training avg: {results_data[algo.upper()]['final_train_avg']:.2f}\")\n",
    "    print(f\"  Evaluation avg (10 episodes): {results_data[algo.upper()]['eval_avg']:.2f} ± {results_data[algo.upper()]['eval_std']:.2f}\")\n",
    "    print(f\"  Inference time per episode: {inference_time:.4f}s\")\n",
    "\n",
    "# Generate comparison table\n",
    "print_results_table(results_data, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e52440",
   "metadata": {},
   "source": [
    "## Generate Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0df0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "plot_learning_curves(results, save_path=f\"{output_dir}/learning_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef50013",
   "metadata": {},
   "source": [
    "## Generate GIFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061b22e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate GIFs\n",
    "print(f\"\\nGenerating GIFs...\")\n",
    "generate_gif(trained_agents[\"DQN\"], ENV_NAME, f\"{output_dir}/dqn_agent.gif\")\n",
    "generate_gif(trained_agents[\"DDQN\"], ENV_NAME, f\"{output_dir}/ddqn_agent.gif\")\n",
    "generate_gif(trained_agents[\"PER\"], ENV_NAME, f\"{output_dir}/per_agent.gif\")\n",
    "\n",
    "print(f\"\\nEXPERIMENT COMPLETE: {TEST_NAME}\")\n",
    "print(f\"All results saved to: {output_dir}/\")\n",
    "print(f\"  - learning_curves.png: Training performance comparison\")\n",
    "print(f\"  - results_table.txt: Quantitative metrics comparison\")\n",
    "print(f\"  - GIF files: Trained agent demonstrations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b20fd3f",
   "metadata": {},
   "source": [
    "## Display Results\n",
    "\n",
    "You can download the generated files from the Files panel on the left, or use the code below to display the GIFs inline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebbeda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Display GIFs\n",
    "print(\"DQN Agent:\")\n",
    "display(Image(filename=f\"{output_dir}/dqn_agent.gif\"))\n",
    "\n",
    "print(\"\\nDDQN Agent:\")\n",
    "display(Image(filename=f\"{output_dir}/ddqn_agent.gif\"))\n",
    "\n",
    "print(\"\\nPER Agent:\")\n",
    "display(Image(filename=f\"{output_dir}/per_agent.gif\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef260d2",
   "metadata": {},
   "source": [
    "## Download Results\n",
    "\n",
    "Run this cell to download all results as a zip file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d878b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the results\n",
    "!zip -r {output_dir}.zip {output_dir}\n",
    "\n",
    "# Download the zip file\n",
    "from google.colab import files\n",
    "files.download(f\"{output_dir}.zip\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
